\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Scientific Computing - Exercise Sheet 1}
\author{Jonathan Hellwig, Jule Sch√ºtt, Mika Tode, Giuliano Taccogna}
\date{\today}

\usepackage{float}
\usepackage{svg}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{enumitem}
\usepackage{amssymb}

\begin{document}

\maketitle

\section{Exercise}
\begin{enumerate}[label=(\alph*)]
\item Noch nichts gemacht:\\

   \textbf{Input}: $ \textbf{A} \in \mathbb{R}^{n\times n} \quad b, x_0\in \mathbb{R}^n$
    \begin{algorithmic}[1]
	\State $h_{0} = \textbf{A}x_{0}$ 
	\State $r_0 = b - h_0$
	\State $p_0 = r_0$
	\State $\beta_0 = r_0^T\cdot r_0$
	\For{$k = 1,2,\dots$}
	\State $h_{k-1} = \textbf{A}p_{k-1}\qquad$ (matrix multiplication)
	\State $\gamma_{k-1} = p^{T}_{k-1}\cdot h_{k-1}\qquad$ (1. loop)
	\State $\alpha_{k-1} = \frac{\beta^{k-1}}{\gamma^{k-1}}$
	\State $x_k = x_{k-1} + \alpha_{k-1}p_{k-1}\qquad$ (2 loop)
	\State $r_k = r_{k-1} - \alpha_{k-1}h_{k-1}\qquad$ (3. loop)
	\State $\beta_k = r_k^T\cdot r_k\qquad \qquad \qquad$ (4. loop)
	\State $p_k = r_{k} + \frac{\beta_k}{\beta_{k-1}}p_{k-1}\qquad$ (5. loop)
	\EndFor
    \end{algorithmic}
    
    
\item
  The idea is to split the vectors such that the first coordinates were computed by one processor and the last coordinates were computed by the second processor. Therefore $i$ denotes the counter, which is needed for the loop operation. Notice that for the Vectormultiplications in the loops 1 and 4 normally also a \textbf{reduce} operation should be included.
  \\
   \textbf{Input}: $ \textbf{A} \in \mathbb{R}^{n\times n} \quad b, x_0\in \mathbb{R}^n$
    \begin{algorithmic}[1]
	\State $h_{0} = \textbf{A}x_{0}$ 
	\State \textbf{begin parallel private$(i, r_{0}$) shared$(b, h_{0})$}
	\State $r_0 = b - h_0$
	\State \textbf{end parallel}
	\State $p_0 = r_0$
	\State \textbf{begin parallel private$(i, \beta_{0}$) shared$(r_0)$}
	\textbf{end parallel}
	\State $\beta_0 = r_0^T\cdot r_0$
	\For{$k = 1,2,\dots$}
	\State $h_{k-1} = \textbf{A}p_{k-1}\qquad$ (matrix multiplication)
	\State \textbf{begin parallel private$(i, \gamma_{k-1}$) shared$(p_{k-1}, h_{k-1})$}
	\State $\gamma_{k-1} = p^{T}_{k-1}\cdot h_{k-1}\qquad$ (1. loop)
	\State \textbf{end parallel}
	\State $\alpha_{k-1} = \frac{\beta^{k-1}}{\gamma^{k-1}}$
	\State \textbf{begin parallel private$(i,x_k, p_k$) shared$(r_{k-1}, r_k,x_{k-1}, h_{k-1},\alpha_{k-1},\beta_k, \beta_{k-1}, p_{k-1})$}
	\State $x_k = x_{k-1} + \alpha_{k-1}p_{k-1}\qquad$ (2 loop)
	\State $r_k = r_{k-1} - \alpha_{k-1}h_{k-1}\qquad$ (3. loop)
	\State $\beta_k = r_k^T\cdot r_k\qquad \qquad \qquad$ (4. loop)
	\State $p_k = r_{k} + \frac{\beta_k}{\beta_{k-1}}p_{k-1}\qquad$ (5. loop)
	\State \textbf{end parallel}
	\EndFor
    \end{algorithmic}
    

\item
We determine that for all parallelizable parts Processor 0 works on the for-loop for $i = 1,...,n/2$ and Processor 1 works on the for-loop for $i = n/2+1,...,n$, with $n/2$ possibly rounded.\\
Accordingly $v^{(j)}$ denotes a scalarproduct or a vector which is calculated by processor $j = 0, 1$. 
If $v^{(j)}$ denotes a Vector we mean $v^{(0)} = (v(1), v(2), ..., v(n/2))^T$ and $v^{(1)} = (v(n/2+1), v(n/2+2), ..., v(n))^T$.
If $v^{(j)}$ denotes a scalarproduct, we mean $v^{(j)} = H^{T(j)} \cdot K^{(j)}$ for $H, K \in \mathbb{R}^n$.\\
If a processor recives data through the \textbf{recv} command, we implicitly include in \textbf{recv} that the recieved data is combined with the data that was calculated on the processor itself in the right way. \\
That means if one half of a vector $v^{(i)}$ is recieved \textbf{recv} creates the whole vector $v=v^{(0)}+v^{(1)}$. If "one half" of a scalarproduct is recived \textbf{recv} adds the corresponding scalarproducts to the final scalarproduct.
If the processor is not specified in a calculation, the calculation is executed on both processors.
\\
For the first processor (index 0):
\\
   \textbf{Input}: $ \textbf{A} \in \mathbb{R}^{n\times n} \quad b, x_0\in \mathbb{R}^n$
    \begin{algorithmic}[1]
	\State $h_{0} = \textbf{A}x_{0}$ 
	\State $r^{(0)}_0 = b^{(0)} - h_0^{(0)}$
	\State \textbf{barrier}
	\State \textbf{send}($r_0^{(0)}$)
	\State \textbf{recv}($r_0^{(1)}$)
	\State $p_0 = r_0$
	\State $\beta_0^{(0)} = r^{(0)}_0^T\cdot r^{(0)}_0$
		\State \textbf{barrier}
	\State \textbf{send}($\beta_0^{(0)}$)
	\State \textbf{recv}($\beta_0^{(1)}$)
	\For{$k = 1,2,\dots$}
	\State $h_{k-1} = \textbf{A}p_{k-1}\qquad$ (matrix multiplication)
	\State $\gamma^{(0)}_{k-1} = (p^{(0)}_{k-1})^T\cdot h^{(0)}_{k-1}\qquad$ (1. loop)
			\State \textbf{barrier}
	\State \textbf{send}($\gamma_{k-1}^{(0)}$)
	\State \textbf{recv}($\gamma_{k-1}^{(1)}$)
	\State $\alpha_{k-1} = \frac{\beta^{k-1}}{\gamma^{k-1}}$
	\State $x^{(0)}_k = x^{(0)}_{k-1} + \alpha^{(0)}_{k-1}p^{(0)}_{k-1}\qquad$ (2 loop)
	\State $r^{(0)}_k = r^{(0)}_{k-1} - \alpha^{(0)}_{k-1}h^{(0)}_{k-1}\qquad$ (3. loop)
	\State $\beta^{(0)}_k = (r^{(0)}_k)^T\cdot r^{(0)}_k\qquad \qquad $ (4. loop)
				\State \textbf{barrier}
	\State \textbf{send}($\beta_k^{(0)}$)
	\State \textbf{recv}($\beta_k^{(1)}$)
	\State $p^{(0)}_k = r^{(0)}_{k} + \frac{\beta^{(0)}_k}{\beta^{(0)}_{k-1}}p_{k-1}\qquad$ (5. loop)
					\State \textbf{barrier}
	\State \textbf{send}($p_k^{(0)}$)
	\State \textbf{recv}($p_k^{(1)}$)
	\EndFor
    \end{algorithmic}

\item



\end{enumerate} 
\end{document}
