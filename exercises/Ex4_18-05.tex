\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Scientific Computing - Exercise Sheet 4}
\author{Jonathan Hellwig, Jule SchÃ¼tt, Mika Tode, Giuliano Taccogna}
\date{\today}

\usepackage{float}
\usepackage{svg}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{amsmath}

\begin{document}

\maketitle

\section{Exercise}
\begin{enumerate}[label=(\alph*)]
\item We have the following CG algorithm this time:\\

   \textbf{Input}: $ \textbf{A} \in \mathbb{R}^{n\times n} \quad b, x_0\in \mathbb{R}^n$
    \begin{algorithmic}[1]
	\State $h_{0} = \textbf{A}x_{0}$ 
	\State $r_0 = b - h_0$
	\State $p_0 = r_0$
	\State $\beta_0 = r_0^T\cdot r_0$
	\For{$k = 1,2,\dots$}
	\State $h_{k-1} = \textbf{A}p_{k-1}\qquad$ (matrix multiplication)
	\State $\gamma_{k-1} = p^{T}_{k-1}\cdot h_{k-1}\qquad$ (1. loop)
	\State $\alpha_{k-1} = \frac{\beta^{k-1}}{\gamma^{k-1}}$
	\State $x_k = x_{k-1} + \alpha_{k-1}p_{k-1}\qquad$ (2. loop)
	\State $r_k = r_{k-1} - \alpha_{k-1}h_{k-1}\qquad$ (3. loop)
	\State $\beta_k = r_k^T\cdot r_k\qquad \qquad \qquad$ (4. loop)
	\State $p_k = r_{k} + \frac{\beta_k}{\beta_{k-1}}p_{k-1}\qquad$ (5. loop)
	\EndFor
    \end{algorithmic}
All sections marked as a loop can be parallelised as well as the lines 2 and 4 (see b)). When considering the CREW-P-RAM machine, we cannot write at the same value with more than one processor at the same time (exclusive writing). Therefore, we have to make sure that in the first and fourth loop and in line 4 (where a scalar is computed!) we only write with one processor.\\
Data dependencies occur in the second, third and fifth loop, as a previous value of the one currently calculated is needed for calculation (e.g. in the second loop $x_{k-1}$ is needed for computation of $x_k$). One has to make sure not to overwrite the old value before computing the next one.



\item
  The idea is to split the vectors such that the first coordinates are computed by one processor and the last coordinates are computed by the second processor. Therefore $i$ denotes the counter, which is needed for the loop operation. Notice that for the Vectormultiplications in the loops 1 and 4 and in line 7 normally also a \textbf{reduce} operation should be included.
  \\
   \textbf{Input}: $ \textbf{A} \in \mathbb{R}^{n\times n} \quad b, x_0\in \mathbb{R}^n$
    \begin{algorithmic}[1]
	\State $h_{0} = \textbf{A}x_{0}$ 
	\State \textbf{begin parallel private$(i, r_{0}$) shared$(b, h_{0})$}
	\State $r_0 = b - h_0$
	\State \textbf{end parallel}
	\State $p_0 = r_0$
	\State \textbf{begin parallel private$(i, \beta_{0}$) shared$(r_0)$}
	\State $\beta_0 = r_0^T\cdot r_0$
	\State \textbf{end parallel}
	\For{$k = 1,2,\dots$}
	\State $h_{k-1} = \textbf{A}p_{k-1}\qquad$ (matrix multiplication)
	\State \textbf{begin parallel private$(i, \gamma_{k-1}$) shared$(p_{k-1}, h_{k-1})$}
	\State $\gamma_{k-1} = p^{T}_{k-1}\cdot h_{k-1}\qquad$ (1. loop)
	\State \textbf{end parallel}
	\State $\alpha_{k-1} = \frac{\beta^{k-1}}{\gamma^{k-1}}$
	\State \textbf{begin parallel private$(i,x_k, r_k, \beta_k$) shared$(r_{k-1},x_{k-1}, h_{k-1},\alpha_{k-1}, p_{k-1})$}
	\State $x_k = x_{k-1} + \alpha_{k-1}p_{k-1}\qquad$ (2. loop)
	\State $r_k = r_{k-1} - \alpha_{k-1}h_{k-1}\qquad$ (3. loop)
	\State $\beta_k = r_k^T\cdot r_k\qquad \qquad \qquad$ (4. loop)
	\State \textbf{end parallel}
	\State \textbf{begin parallel private$(i, p_k$) shared$(r_{k}, \beta_{k-1}, \beta_{k}, p_{k-1})$}
	\State $p_k = r_{k} + \frac{\beta_k}{\beta_{k-1}}p_{k-1}\qquad$ (5. loop)
	\State \textbf{end parallel}
	\EndFor
    \end{algorithmic}


\item
We determine that for all parallelizable parts Processor 0 works on the for-loop for $i = 1,...,n/2$ and Processor 1 works on the for-loop for $i = n/2+1,...,n$, with $n/2$ possibly rounded.
\\
For fulfilling the scalar product one has to include a command that adds all summands which were computed on different processors after the parallelized computation (for example in line 7).
\\
Pseudocode for the processor with index $i\in\{0,1\}$:
\\
   \textbf{Input}: $ \textbf{A} \in \mathbb{R}^{n\times n} \quad b, x_0\in \mathbb{R}^n$
    \begin{algorithmic}[1]
	\State $h_{0} = \textbf{A}x_{0}\qquad $ (matrix multiplication parallel with processor $i-1$)
	\State $r^{(i)}_0 = b^{(i)} - h_0^{(i)}$
	\State $p_0^{i} = r_0^{i}$
	\State $\beta_0^{(i)} = r^{(i)T}_0\cdot r^{(i)}_0$
	\State \textbf{send}($\beta_{0}^{(i)}$)
	\State \textbf{recv}($\beta_{0}^{(1-i)}$)
	\State $\beta_0=\beta_0^{(i)}+\beta_0^{(1-i)}$
	\For{$k = 1,2,\dots$}
	\State $h^{(i)}_{k-1} = \textbf{A}p^{(i)}_{k-1}\quad$ \small(matrix multiplication parallel with processor $i-1$)\normalsize
	\State $\gamma^{(i)}_{k-1} = (p^{(i)}_{k-1})^T\cdot h^{(i)}_{k-1}\qquad$ (1. loop)
	\State \textbf{send}($\gamma_{k-1}^{(i)}$)
	\State \textbf{recv}($\gamma_{k-1}^{(1-i)}$)
	\State $\gamma_{k-1} = \gamma_{k-1}^{(i)}+\gamma_{k-1}^{(1-i)}$
	\State $\alpha_{k-1} = \frac{\beta_{k-1}}{\gamma_{k-1}}$
	\State $x^{(i)}_k = x^{(i)}_{k-1} + \alpha_{k-1}p^{(i)}_{k-1}\qquad$ (2 loop)
	\State $r^{(i)}_k = r^{(i)}_{k-1} - \alpha_{k-1}h^{(i)}_{k-1}\qquad$ (3. loop)
	\State $\beta^{(i)}_k = (r^{(i)}_k)^T\cdot r^{(i)}_k\qquad \qquad $ (4. loop)
	\State \textbf{send}($\beta_k^{(i)}$)
	\State \textbf{recv}($\beta_k^{(1-i)}$)
	\State $\beta_k = \beta_k^{(i)}+\beta_k^{(1-i)}$
	\State $p^{(i)}_k = r^{(i)}_{k} + \frac{\beta_k}{\beta_{k-1}}p_{k-1}^{(i)}\qquad$ (5. loop)
	\State \textbf{barrier}
	\EndFor
    \end{algorithmic}

\item  We remember the solution of the algorithm of exercise sheet 3 for one processor 0 here:
 \textbf{Input}: $ \textbf{A} \in \mathbb{R}^{n\times n} \quad b, x_0\in \mathbb{R}^n$ 
    \begin{algorithmic}[1]
	\State $h^{(0)}_{0} = \textbf{A}x_{0}^{(0)}$ (matrix multiplication in unknown parallelization)
	\State $r_0^{(0)} = b^{(0)} - h_0^{(0)}$ (1.1 loop)
	\State $p_0^{(0)} = r_0^{(0)}$
	\For{$k = 1,2,\dots$}
	\State $h_{k-1}^{(0)} = \textbf{A}p_{k-1}^{(0)}$ (matrix multiplication)
	\State $\gamma_{k-1}^{(0)} = p^{T(0)}_{k-1}\cdot h_{k-1}^{(0)}$ (2.1 loop)
	\State $\beta_{k-1}^{(0)} = r^{T(0)}_{k-1}\cdot r_{k-1}^{(0)}$ (2.2 loop)
	\State $\delta_{k-1}^{(0)} = r^{T(0)}_{k-1}\cdot h_{k-1}^{(0)}$ (2.3 loop)
	\State $\zeta_{k-1}^{(0)} = h^{T(0)}_{k-1}\cdot h_{k-1}^{(0)}$ (2.4 loop)
      	\State \textbf{send($\gamma_{k-1}^{(0)}, \beta_{k-1}^{(0)}, \delta_{k-1}^{(0)}, \zeta_{k-1}^{(0)}$)}
      	\State \textbf{recv($\gamma_{k-1}^{(1)}, \beta_{k-1}^{(1)}, \delta_{k-1}^{(1)}, \zeta_{k-1}^{(1)}$)}
      	\State $\gamma_{k-1}=\gamma_{k-1}^{(0)} + \gamma_{k-1}^{(1)}$
      	\State $\beta_{k-1}=\beta_{k-1}^{(0)} + \beta_{k-1}^{(1)}$
      	\State $\delta_{k-1}=\delta_{k-1}^{(0)} + \delta_{k-1}^{(1)}$
      	\State $\zeta_{k-1}=\zeta_{k-1}^{(0)} + \zeta_{k-1}^{(1)}$
	\State $\alpha_{k-1} = \frac{\beta^{k-1}}{\gamma^{k-1}}$
	\State $\beta_{k} = \beta_{k-1} - 2 \alpha_{k-1} \delta_{k-1} + \alpha_{k-1}^2\zeta_{k-1}$
	\State $x_k^{(0)} = x_{k-1}^{(0)} + \alpha_{k-1}p_{k-1}^{(0)}$ (2.5 loop)
	\State $r_k^{(0)} = r_{k-1}^{(0)} - \alpha_{k-1}h_{k-1}^{(0)}$ (2.6 loop)
	\State $p_k^{(0)} = r_{k}^{(0)} + \frac{\beta_k}{\beta_{k-1}}p_{k-1}^{(0)}$ (2.7 loop)
	\State \textbf{barrier}
	\EndFor
    \end{algorithmic} 
To compare the performance of the algorithm of exercise sheet 3 and 4 we need to specify the paradigm. We are looking at the message passing model, so we have to consider exercise c) of both sheets.
\\
The CG algorithm runs for 100 seconds on one processor, i.e. 
\begin{align*}
    t_1^N=100.
\end{align*}
Thus,
\begin{align*}
    t_p^N=\frac{100}{p}.
\end{align*}
Further the communication time is given by
\begin{align*}
    t=0.01
\end{align*}
per processor and send/rec pair. We consider $p\in\{10,50,100\}$.
\\
We are mainly looking on the for loop in both algorithm since we don't know how many times we have to do the for loop we can not compute the serial program fraction $\nu$ in a suitable way while looking at the start part. Anyway, the for loop needs much more time and we can disregard the other part.
\\
For the first algorithm on exercise sheet 3 we count 6 of 14 instructions which can not be parallelized, so 
\begin{align*}
    \nu^1=\frac{6}{14}=\frac{3}{7}.
\end{align*}
For the second algorithm we count 3 of 9 parts which can not be parallelized, so
\begin{align*}
    \nu^2=\frac{3}{9}=\frac{1}{3}.
\end{align*}
Since $\nu^1>\nu^2$, we expect that the second algorithm is faster and more effective.
\\
The amount of pairs of send/rec can be counted: $x^1=1,\;x^2=2$. Hence, the total communication overhead is given by
\begin{align*}
    t^i_c=tx^ip\qquad i\in\{1,2\}.
\end{align*}
Finally we can compute the total run time and the speedup and efficiency by the following formulas:
\begin{align*}
    (t_{total}^\nu)^i&= \nu^i t_1^N +(1-\nu^i)t_p^N\\
    S_p^i &= \frac{t_1^N}{(t_{total}^N)^i+t_c^i}\\
    E_p^i &= \frac{t_1^N}{p((t_{total}^\nu)^i+t_c^i)}.
\end{align*}
We approximate the numbers to the second decimal place:
    \begin{center}
\begin{tabular}{ c | c | c | c } 
  $p$ & $(t_{total}^\nu)^1$ & $S_p^1$ & $E_p^1$ \\
  \hline
  10    &    48,571             &   2,055  &   0,205\\
  50    &    $44$    &   2,247 &   0,045   \\
  100   &    43,429             &   2,251           &   0,023    \\
  \\
  $p$ & $(t_{total}^\nu)^2$ & $S_p^2$ & $E_p^2$ \\
  \hline
  10    &    40    &   2,488 &      0,249 \\
  50    &    34,667             &   2,804 &  0,056   \\
  100   &    34    &   2,778&       0,028    \\
\end{tabular}        
    \end{center}

The second algorithm needs a smaller computing time (total run time) throughout the different numbers of processors compared to the first algorithm, which aligns with our expectation and the fact that a larger fraction of the second algorithm can be parallelized. Furthermore, the effective speedup and efficieny for the second algorithm is larger than the effective speedup and efficiency values for Algorithm 1 for each number of processors p.\\
As we have seen on the second exercise sheet, adding more and more processors does not always increase effective speedup and efficiency infinitely, e.g. for $p=50$ we have a higher speedup in both algorithms compared to their respective speedup and efficiency values for $p=100$.
\end{enumerate} 
\end{document}